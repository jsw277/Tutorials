{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automation in Data Cleansing \n",
    "## Tutorial by *Nikos Gavriil*\n",
    "-----------------------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we are going to talk about automation in the Data Cleansing process. Data cleansing (or data cleaning) is defined in Wikipedia as:\n",
    "\n",
    ">The process of detecting and correcting (or removing) corrupt or inaccurate records from a record set, table, or database and refers to identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying, or deleting the dirty or coarse data.  \n",
    "\n",
    "So the question is...can it be automated? Yes! Should I do it? To answer this question let me tell you some things about the algorithm to be presented in this project. In order to clean up a dataset you can't just press pandas.DataFrame.dropna()and make the missing values disappear. Actually you can, but the result may not be what you were expecting in the sense that if some columns are full of missing values the whole dataset will be erased. Actually it takes only one missing value per row to delete the whole dataset, so there are some actions to be taken before using the dropna() function. These actions could be:\n",
    "\n",
    "* _Identifying the columns that are full (or in their largest part full) of NAs and removing them first._ After that if we use the dropna() function less or equal rows will be deleted.\n",
    "* _Identying useless columns with respect to some task._ For example if I would like to predict one column using the others I could use some criteria to choose the most relevant columns.\n",
    "\n",
    "As you see, cleaning a dataset includes making some assumptions for the relationship between the features it contains. So other process should be followed if someone expects a linear or nonlinear relationship. The algorithm that will be presented here deals with linear relationship between a variable that will be chosen by the user as dependent and the features that will be used for predicting the dependent variable. So let's move to the mechanics of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#the following python modules are needed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RandomizedLasso\n",
    "from sklearn.linear_model import RandomizedLogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down the main parts of the algorithm..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Preparation\n",
    "\n",
    "In the first part we prepare the data. Drop the columns that are over 90% covered in missing values, transform the categorical features from strings to integers and standardize the columns of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "frame.drop(list(frame.isnull().sum()[frame.isnull().sum()>=int(0.9*frame.shape[0])].index.values),axis=1,inplace=True)\n",
    "fr = frame.dropna()\n",
    "for col in fr.columns:\n",
    "    if ((fr[col].dtype != np.float64) & (fr[col].dtype != np.int64)):\n",
    "        fr[col] = le.fit_transform(fr[col])\n",
    "scaler = StandardScaler()\n",
    "if type(predcolumn)==int: \n",
    "    X=fr.drop(fr.columns[predcolumn],axis=1)\n",
    "elif type(predcolumn)==str:\n",
    "    X=fr.drop(predcolumn,axis=1)\n",
    "X = scaler.fit_transform(X)\n",
    "if type(predcolumn)==int:\n",
    "    y= fr[fr.columns[predcolumn]]\n",
    "elif type(predcolumn)==str:\n",
    "    y= fr[predcolumn]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Relevant features for regression\n",
    "If the action asked is regression then the algorithm will perform stability selection using Randomised Lasso and keep only the relevant features. Of course we can choose the sensitivity of the algorithm with the alphareg parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if regression==True:\n",
    "    model = RandomizedLasso(alpha=alphareg,random_state=1)\n",
    "    model.fit(X,y)\n",
    "    filterdcols = model.get_support()\n",
    "    filterdcols = pd.Series(filterdcols).append(pd.Series(False),ignore_index=True)\n",
    "    if type(predcolumn)==int:\n",
    "        framenew = pd.concat([frame[frame.columns[filterdcols]],frame[frame.columns[predcolumn]]],axis=1)\n",
    "    elif type(predcolumn)==str:\n",
    "        framenew = pd.concat([frame[frame.columns[filterdcols]],frame[predcolumn]],axis=1)\n",
    "    framenew = framenew.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Relevant features for classification\n",
    "The same philosophy can be applied if the task to be performed is classification. This time the sensitivity is calibrated by the Cclass parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = RandomizedLogisticRegression(C=Cclass,random_state=1)\n",
    "model.fit(X,y)\n",
    "filterdcols = model.get_support()\n",
    "filterdcols = pd.Series(filterdcols).append(pd.Series(False),ignore_index=True)\n",
    "if type(predcolumn)==int:\n",
    "    framenew = pd.concat([frame[frame.columns[filterdcols]],frame[frame.columns[predcolumn]]],axis=1)\n",
    "elif type(predcolumn)==str:\n",
    "    framenew = pd.concat([frame[frame.columns[filterdcols]],frame[predcolumn]],axis=1)\n",
    "framenew = framenew.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So all you have to do is choose the variable that you would like to predict, the task to be performed (regression or classification) and the sensitivity of the algorithm (controlling how many columns will be left). Connecting all the previous parts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleaner(frame,predcolumn,alphareg ='aic',regression=True,Cclass=1):\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    le = LabelEncoder()\n",
    "    frame.drop(list(frame.isnull().sum()[frame.isnull().sum()>=int(0.9*frame.shape[0])].index.values),axis=1,inplace=True)\n",
    "    fr = frame.dropna()\n",
    "    for col in fr.columns:\n",
    "        if ((fr[col].dtype != np.float64) & (fr[col].dtype != np.int64)):\n",
    "            fr[col] = le.fit_transform(fr[col])\n",
    "    scaler = StandardScaler()\n",
    "    if type(predcolumn)==int:\n",
    "        X=fr.drop(fr.columns[predcolumn],axis=1)\n",
    "    elif type(predcolumn)==str:\n",
    "        X=fr.drop(predcolumn,axis=1)\n",
    "    X = scaler.fit_transform(X)\n",
    "    if type(predcolumn)==int:\n",
    "        y= fr[fr.columns[predcolumn]]\n",
    "    elif type(predcolumn)==str:\n",
    "        y= fr[predcolumn]\n",
    "    if regression==True:\n",
    "        model = RandomizedLasso(alpha=alphareg,random_state=1)\n",
    "        model.fit(X,y)\n",
    "        filterdcols = model.get_support()\n",
    "        filterdcols = pd.Series(filterdcols).append(pd.Series(False),ignore_index=True)\n",
    "        if type(predcolumn)==int:\n",
    "            framenew = pd.concat([frame[frame.columns[filterdcols]],frame[frame.columns[predcolumn]]],axis=1)\n",
    "        elif type(predcolumn)==str:\n",
    "            framenew = pd.concat([frame[frame.columns[filterdcols]],frame[predcolumn]],axis=1)\n",
    "        framenew = framenew.dropna()\n",
    "    else:\n",
    "        model = RandomizedLogisticRegression(C=Cclass,random_state=1)\n",
    "        model.fit(X,y)\n",
    "        filterdcols = model.get_support()\n",
    "        filterdcols = pd.Series(filterdcols).append(pd.Series(False),ignore_index=True)\n",
    "        if type(predcolumn)==int:\n",
    "            framenew = pd.concat([frame[frame.columns[filterdcols]],frame[frame.columns[predcolumn]]],axis=1)\n",
    "        elif type(predcolumn)==str:\n",
    "            framenew = pd.concat([frame[frame.columns[filterdcols]],frame[predcolumn]],axis=1)\n",
    "        framenew = framenew.dropna()\n",
    "    return framenew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Main Page](https://nikosga.github.io/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
